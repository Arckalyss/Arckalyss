{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAZphJ2Wd45V4bt4bHFcRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arckalyss/Arckalyss/blob/main/ClinicalBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "T√©l√©chargement des biblioth√®ques"
      ],
      "metadata": {
        "id": "-lUUGHNYoVb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_0tsQWXn_o-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets scikit-learn lime captum\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montage du drive et chargement des mod√®les fine tun√©s"
      ],
      "metadata": {
        "id": "2hDKi8cEobUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G04jK2lloNdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/clinicalbert_diabetes /content/"
      ],
      "metadata": {
        "id": "uvsZpt40oROZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/diabetes_clinical_notes.csv /content/"
      ],
      "metadata": {
        "id": "lfapqohLoSkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "KAyaXgJaoqLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "print(df.iloc[0][\"TEXT\"][:500])"
      ],
      "metadata": {
        "id": "Q3Wo2SJloq-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pr√©paration de donn√©es m√©dicales pour Bio_ClinicalBERT"
      ],
      "metadata": {
        "id": "mEFsc2BlorkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "\n",
        "class ClinicalNotesDataset(Dataset):\n",
        "    def __init__(self, csv_path, max_length=256):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.texts = self.data[\"TEXT\"].astype(str).tolist()\n",
        "        self.labels = self.data[\"label\"].tolist()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item"
      ],
      "metadata": {
        "id": "zEqb7tg_pBaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrainement de Bio ClinicalBert pour la classification du diab√®te"
      ],
      "metadata": {
        "id": "880CganRpGOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "dataset = ClinicalNotesDataset(DATA_PATH)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=2\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            preds.extend(predictions.cpu().numpy())\n",
        "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"binary\"\n",
        "    )\n",
        "\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"F1-score : {f1:.4f}\")\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Validation results:\")\n",
        "    evaluate(model, val_loader)\n",
        "\n",
        "model.save_pretrained(\"/content/clinicalbert_diabetes\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(\"/content/clinicalbert_diabetes\")"
      ],
      "metadata": {
        "id": "ZQNqXA37pFW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test du fine-tuning"
      ],
      "metadata": {
        "id": "LASBMPlDpPbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# üîπ Chemin vers votre mod√®le pr√©-entra√Æn√©\n",
        "MODEL_PATH = \"/content//clinicalbert_diabetes\"\n",
        "\n",
        "# üîπ V√©rifie si GPU est disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# üîπ Charger le mod√®le et le tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()  # mode √©valuation\n",
        "\n",
        "# üîπ Phrase √† tester\n",
        "test_text = \"Patient diagnosed with type 2 diabetes, HbA1c 8.5%, insulin therapy initiated.\"\n",
        "\n",
        "# üîπ Tokenisation\n",
        "inputs = tokenizer(\n",
        "    [test_text],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "# üîπ Pr√©diction du mod√®le (logits)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "predicted_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "# üîπ Affichage des r√©sultats\n",
        "print(\"Texte test√© :\", test_text)\n",
        "print(\"Logits :\", logits.cpu().numpy())\n",
        "print(\"Probabilit√©s :\", probs.cpu().numpy())\n",
        "print(\"Classe pr√©dite :\", predicted_class.item())\n",
        "print(model.config.id2label)"
      ],
      "metadata": {
        "id": "lN_NI9NspOyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mise en oeuvre de SHAP pour une phrase r√©f√©rence"
      ],
      "metadata": {
        "id": "aIWkKQDepaal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shap\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"/content/clinicalbert_diabetes\"\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load model & tokenizer\n",
        "# -----------------------------\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Load dataset\n",
        "# -----------------------------\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Nettoyage essentiel\n",
        "df = df.dropna(subset=[\"TEXT\"])\n",
        "df[\"TEXT\"] = df[\"TEXT\"].astype(str)\n",
        "\n",
        "# √âchantillon pour SHAP (d√©but petit pour debug)\n",
        "texts = df[\"TEXT\"].sample(5, random_state=42).tolist()\n",
        "texts = [\"Patient diagnosed with type 2 diabetes, HbA1c 8.5%, insulin therapy initiated.\"]\n",
        "\n",
        "# V√©rification\n",
        "print(f\"Type du premier texte : {type(texts[0])}\")\n",
        "print(f\"Premier texte : {texts[0][:200]}\")  # affiche les 200 premiers caract√®res\n",
        "\n",
        "# -----------------------------\n",
        "# Fonction predict_proba s√©curis√©e\n",
        "# -----------------------------\n",
        "def predict_proba(texts):\n",
        "    # Normalisation pour SHAP (peut recevoir str ou list)\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    else:\n",
        "        texts = list(texts)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Cr√©ation de l'explainer SHAP\n",
        "# -----------------------------\n",
        "print(\"Creating explainer...\")\n",
        "masker = shap.maskers.Text(tokenizer)  # Indispensable pour Transformers\n",
        "explainer = shap.Explainer(predict_proba, masker)\n",
        "\n",
        "# -----------------------------\n",
        "# Calcul des SHAP values\n",
        "# -----------------------------\n",
        "print(\"Computing SHAP values...\")\n",
        "shap_values = explainer(texts)\n",
        "\n",
        "# -----------------------------\n",
        "# Affichage d'un exemple\n",
        "# -----------------------------\n",
        "print(\"Displaying explanation for one patient...\")\n",
        "shap.plots.text(shap_values[0])"
      ],
      "metadata": {
        "id": "eOp-_7uOpfYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mis en oeuvre de SHAP pour un jeu de donn√©es"
      ],
      "metadata": {
        "id": "zwJJ2ERLpuCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shap\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# Params\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"/content/clinicalbert_diabetes\"\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "MAX_LENGTH = 256\n",
        "TOP_K = 30\n",
        "SAMPLE_SIZE = 10\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Charger mod√®le et tokenizer\n",
        "# -----------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Charger dataset\n",
        "# -----------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(subset=[\"TEXT\"])\n",
        "df[\"TEXT\"] = df[\"TEXT\"].astype(str)\n",
        "texts = df[\"TEXT\"].tolist()\n",
        "\n",
        "# -----------------------------\n",
        "# Stopwords & ponctuation\n",
        "# -----------------------------\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# -----------------------------\n",
        "# Fonction predict_proba\n",
        "# -----------------------------\n",
        "def predict_proba(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    else:\n",
        "        texts = list(texts)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# SHAP Explainer\n",
        "# -----------------------------\n",
        "masker = shap.maskers.Text(tokenizer)\n",
        "explainer = shap.Explainer(predict_proba, masker, output_names=[\"class0\", \"class1\"])\n",
        "\n",
        "# -----------------------------\n",
        "# √âchantillon repr√©sentatif\n",
        "# -----------------------------\n",
        "sample_texts = random.sample(texts, min(SAMPLE_SIZE, len(texts)))\n",
        "\n",
        "# -----------------------------\n",
        "# Calcul SHAP values\n",
        "# -----------------------------\n",
        "shap_values = explainer(sample_texts)\n",
        "\n",
        "# -----------------------------\n",
        "# Fonction robuste d‚Äôagr√©gation via offsets\n",
        "# -----------------------------\n",
        "def aggregate_using_offsets(text, shap_values_instance):\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_offsets_mapping=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
        "\n",
        "    values_class0 = np.abs(shap_values_instance.values[:, 0])\n",
        "    values_class1 = np.abs(shap_values_instance.values[:, 1])\n",
        "\n",
        "    word_dict_class0 = defaultdict(float)\n",
        "    word_dict_class1 = defaultdict(float)\n",
        "\n",
        "    current_word = \"\"\n",
        "    current_v0 = 0.0\n",
        "    current_v1 = 0.0\n",
        "    previous_end = None\n",
        "\n",
        "    for token, (start, end), v0, v1 in zip(tokens, offsets, values_class0, values_class1):\n",
        "\n",
        "        if token in tokenizer.all_special_tokens:\n",
        "            continue\n",
        "\n",
        "        if start == end:\n",
        "            continue\n",
        "\n",
        "        piece = text[start:end]\n",
        "\n",
        "        # Nouveau mot si espace ou discontinuit√©\n",
        "        if previous_end is not None and start != previous_end:\n",
        "            word_clean = current_word.lower().strip()\n",
        "            if (\n",
        "                word_clean not in stop_words and\n",
        "                word_clean.isalpha() and\n",
        "                word_clean not in punctuation\n",
        "            ):\n",
        "                word_dict_class0[word_clean] += current_v0\n",
        "                word_dict_class1[word_clean] += current_v1\n",
        "\n",
        "            current_word = piece\n",
        "            current_v0 = v0\n",
        "            current_v1 = v1\n",
        "        else:\n",
        "            current_word += piece\n",
        "            current_v0 += v0\n",
        "            current_v1 += v1\n",
        "\n",
        "        previous_end = end\n",
        "\n",
        "    # Ajouter dernier mot\n",
        "    if current_word:\n",
        "        word_clean = current_word.lower().strip()\n",
        "        if (\n",
        "            word_clean not in stop_words and\n",
        "            word_clean.isalpha() and\n",
        "            word_clean not in punctuation\n",
        "        ):\n",
        "            word_dict_class0[word_clean] += current_v0\n",
        "            word_dict_class1[word_clean] += current_v1\n",
        "\n",
        "    return word_dict_class0, word_dict_class1\n",
        "\n",
        "# -----------------------------\n",
        "# Agr√©gation globale\n",
        "# -----------------------------\n",
        "word_importance_class0 = defaultdict(float)\n",
        "word_importance_class1 = defaultdict(float)\n",
        "\n",
        "for text, sv in zip(sample_texts, shap_values):\n",
        "\n",
        "    w0, w1 = aggregate_using_offsets(text, sv)\n",
        "\n",
        "    for word, val in w0.items():\n",
        "        word_importance_class0[word] += val\n",
        "\n",
        "    for word, val in w1.items():\n",
        "        word_importance_class1[word] += val\n",
        "\n",
        "# -----------------------------\n",
        "# Trier top mots\n",
        "# -----------------------------\n",
        "sorted_class0 = sorted(word_importance_class0.items(), key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "sorted_class1 = sorted(word_importance_class1.items(), key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "\n",
        "# -----------------------------\n",
        "# Visualisation\n",
        "# -----------------------------\n",
        "tokens0, values0 = zip(*sorted_class0)\n",
        "tokens1, values1 = zip(*sorted_class1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,8))\n",
        "\n",
        "axes[0].barh(tokens0[::-1], values0[::-1], color='skyblue')\n",
        "axes[0].set_title(\"Top mots classe 0\")\n",
        "axes[0].set_xlabel(\"Importance globale (|SHAP value|)\")\n",
        "\n",
        "axes[1].barh(tokens1[::-1], values1[::-1], color='salmon')\n",
        "axes[1].set_title(\"Top mots classe 1\")\n",
        "axes[1].set_xlabel(\"Importance globale (|SHAP value|)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Tableau comparatif\n",
        "# -----------------------------\n",
        "df_compare = pd.DataFrame({\n",
        "    \"Classe 0 mots\": tokens0,\n",
        "    \"Classe 0 importance\": values0,\n",
        "    \"Classe 1 mots\": tokens1,\n",
        "    \"Classe 1 importance\": values1\n",
        "})\n",
        "\n",
        "print(df_compare)"
      ],
      "metadata": {
        "id": "rCn2VFmTptm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mise en oeuvre de Lime pour un texte r√©f√©rence"
      ],
      "metadata": {
        "id": "47apFNI-p8uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"/content/clinicalbert_diabetes\"\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load model & tokenizer\n",
        "# -----------------------------\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Phrase √† expliquer (local)\n",
        "# -----------------------------\n",
        "text = \"Patient diagnosed with type 2 diabetes, HbA1c 8.5%, insulin therapy initiated.\"\n",
        "\n",
        "print(f\"\\nTexte analys√© :\\n{text}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Fonction predict_proba (compatible LIME)\n",
        "# -----------------------------\n",
        "def predict_proba(texts):\n",
        "\n",
        "    # LIME peut envoyer une liste numpy\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    else:\n",
        "        texts = list(texts)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Cr√©ation de l'explainer LIME\n",
        "# -----------------------------\n",
        "print(\"Creating LIME explainer...\")\n",
        "\n",
        "class_names = [\"class0\", \"class1\"]\n",
        "\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=class_names\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Explication locale\n",
        "# -----------------------------\n",
        "print(\"Computing LIME explanation...\")\n",
        "\n",
        "explanation = explainer.explain_instance(\n",
        "    text,\n",
        "    predict_proba,\n",
        "    num_features=15,      # nombre de mots affich√©s\n",
        "    num_samples=2000      # nombre de perturbations\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Affichage des r√©sultats\n",
        "# -----------------------------\n",
        "print(\"\\nProbabilit√©s du mod√®le :\")\n",
        "probs = predict_proba(text)[0]\n",
        "print(f\"class0: {probs[0]:.4f}\")\n",
        "print(f\"class1: {probs[1]:.4f}\")\n",
        "\n",
        "print(\"\\nTop mots explicatifs pour la classe pr√©dite :\")\n",
        "predicted_class = np.argmax(probs)\n",
        "\n",
        "for word, score in explanation.as_list(label=predicted_class):\n",
        "    print(f\"{word:<15} {score:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Visualisation Notebook\n",
        "# -----------------------------\n",
        "explanation.show_in_notebook(text=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Sauvegarde HTML\n",
        "# -----------------------------\n",
        "explanation.save_to_file(\"lime_local_explanation.html\")"
      ],
      "metadata": {
        "id": "FUtgQtNap_f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mise en oeuvre de LIME pour un jeu de donn√©es"
      ],
      "metadata": {
        "id": "ye7XU3mBqCpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# -----------------------------\n",
        "# Params\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"/content/clinicalbert_diabetes\"\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "MAX_LENGTH = 256\n",
        "TOP_K = 30\n",
        "SAMPLE_SIZE = 10\n",
        "NUM_SAMPLES_LIME = 1000  # 500 si CPU, 1000-2000 si GPU\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Load model & tokenizer\n",
        "# -----------------------------\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Load dataset\n",
        "# -----------------------------\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(subset=[\"TEXT\"])\n",
        "df[\"TEXT\"] = df[\"TEXT\"].astype(str)\n",
        "texts = df[\"TEXT\"].tolist()\n",
        "\n",
        "# -----------------------------\n",
        "# Stopwords & ponctuation\n",
        "# -----------------------------\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# -----------------------------\n",
        "# Fonction predict_proba\n",
        "# -----------------------------\n",
        "def predict_proba(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    else:\n",
        "        texts = list(texts)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# LIME Explainer\n",
        "# -----------------------------\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=[\"class0\", \"class1\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# √âchantillon repr√©sentatif\n",
        "# -----------------------------\n",
        "sample_texts = random.sample(texts, min(SAMPLE_SIZE, len(texts)))\n",
        "\n",
        "# -----------------------------\n",
        "# Agr√©gation globale LIME\n",
        "# -----------------------------\n",
        "word_importance_class0 = defaultdict(float)\n",
        "word_importance_class1 = defaultdict(float)\n",
        "\n",
        "print(\"Computing LIME global explanations...\")\n",
        "\n",
        "for text in sample_texts:\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        text,\n",
        "        predict_proba,\n",
        "        num_features=TOP_K,\n",
        "        num_samples=NUM_SAMPLES_LIME,\n",
        "        labels=[0, 1]   # IMPORTANT pour √©viter KeyError\n",
        "    )\n",
        "\n",
        "    for label in explanation.local_exp.keys():\n",
        "\n",
        "        for word, score in explanation.as_list(label=label):\n",
        "\n",
        "            word_clean = word.lower().strip()\n",
        "\n",
        "            if (\n",
        "                word_clean not in stop_words and\n",
        "                word_clean.isalpha() and\n",
        "                word_clean not in punctuation\n",
        "            ):\n",
        "                if label == 0:\n",
        "                    word_importance_class0[word_clean] += abs(score)\n",
        "                elif label == 1:\n",
        "                    word_importance_class1[word_clean] += abs(score)\n",
        "\n",
        "# -----------------------------\n",
        "# Trier top mots\n",
        "# -----------------------------\n",
        "sorted_class0 = sorted(\n",
        "    word_importance_class0.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")[:TOP_K]\n",
        "\n",
        "sorted_class1 = sorted(\n",
        "    word_importance_class1.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")[:TOP_K]\n",
        "\n",
        "# -----------------------------\n",
        "# Visualisation\n",
        "# -----------------------------\n",
        "tokens0, values0 = zip(*sorted_class0)\n",
        "tokens1, values1 = zip(*sorted_class1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "axes[0].barh(tokens0[::-1], values0[::-1], color='skyblue')\n",
        "axes[0].set_title(\"Top mots classe 0 (LIME global)\")\n",
        "axes[0].set_xlabel(\"Importance globale (|LIME score|)\")\n",
        "\n",
        "axes[1].barh(tokens1[::-1], values1[::-1], color='salmon')\n",
        "axes[1].set_title(\"Top mots classe 1 (LIME global)\")\n",
        "axes[1].set_xlabel(\"Importance globale (|LIME score|)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Tableau comparatif\n",
        "# -----------------------------\n",
        "df_compare_lime = pd.DataFrame({\n",
        "    \"Classe 0 mots\": tokens0,\n",
        "    \"Classe 0 importance\": values0,\n",
        "    \"Classe 1 mots\": tokens1,\n",
        "    \"Classe 1 importance\": values1\n",
        "})\n",
        "\n",
        "print(\"\\nTop features LIME global :\")\n",
        "print(df_compare_lime)"
      ],
      "metadata": {
        "id": "ILkHxPJBqGpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mise en oeuvre de integrated gradients pour une phrase r√©f√©rence"
      ],
      "metadata": {
        "id": "RpF1lBsPqQ-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Param√®tres\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"/content/clinicalbert_diabetes\"\n",
        "MAX_LENGTH = 256\n",
        "n_steps = 50\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Charger mod√®le et tokenizer\n",
        "# -----------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "# Texte local\n",
        "text = \"Patient diagnosed with type 2 diabetes, HbA1c 8.5%, insulin therapy initiated.\"\n",
        "\n",
        "# Encodage texte\n",
        "encoding = tokenizer(\n",
        "    text,\n",
        "    truncation=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors=\"pt\",\n",
        "    return_offsets_mapping=True\n",
        ").to(device)\n",
        "\n",
        "input_ids = encoding[\"input_ids\"]\n",
        "attention_mask = encoding[\"attention_mask\"]\n",
        "offsets = encoding[\"offset_mapping\"][0].tolist()\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "# -----------------------------\n",
        "# Wrapper forward pour Integrated Gradients\n",
        "# -----------------------------\n",
        "embeddings = model.get_input_embeddings()  # BERT embeddings\n",
        "\n",
        "def forward_func_emb(input_embeds, attention_mask):\n",
        "    outputs = model(inputs_embeds=input_embeds, attention_mask=attention_mask)\n",
        "    return outputs.logits\n",
        "\n",
        "ig = IntegratedGradients(forward_func_emb)\n",
        "\n",
        "# Convertir input_ids ‚Üí embeddings\n",
        "input_embeds = embeddings(input_ids)\n",
        "\n",
        "# -----------------------------\n",
        "# Calcul IG pour chaque classe\n",
        "# -----------------------------\n",
        "local_attr = {}\n",
        "for class_idx in range(model.config.num_labels):\n",
        "    attributions = ig.attribute(\n",
        "        inputs=input_embeds,\n",
        "        additional_forward_args=attention_mask,\n",
        "        target=class_idx,\n",
        "        n_steps=n_steps\n",
        "    ).squeeze(0)\n",
        "\n",
        "    token_attr = attributions.sum(dim=-1).detach().cpu().numpy()\n",
        "\n",
        "    # Agr√©gation par mot\n",
        "    word_dict = defaultdict(float)\n",
        "    current_word = \"\"\n",
        "    current_val = 0.0\n",
        "    previous_end = None\n",
        "\n",
        "    for token, (start, end), val in zip(tokens, offsets, token_attr):\n",
        "        if token in tokenizer.all_special_tokens or start == end:\n",
        "            continue\n",
        "        piece = text[start:end]\n",
        "        if previous_end is not None and start != previous_end:\n",
        "            word_clean = current_word.lower().strip()\n",
        "            if word_clean.isalpha() and word_clean not in stop_words and word_clean not in punctuation:\n",
        "                word_dict[word_clean] += current_val\n",
        "            current_word = piece\n",
        "            current_val = val\n",
        "        else:\n",
        "            current_word += piece\n",
        "            current_val += val\n",
        "        previous_end = end\n",
        "\n",
        "    if current_word:\n",
        "        word_clean = current_word.lower().strip()\n",
        "        if word_clean.isalpha() and word_clean not in stop_words and word_clean not in punctuation:\n",
        "            word_dict[word_clean] += current_val\n",
        "\n",
        "    local_attr[class_idx] = word_dict\n",
        "\n",
        "# -----------------------------\n",
        "# Affichage barplot\n",
        "# -----------------------------\n",
        "for class_idx in local_attr:\n",
        "    word_dict = local_attr[class_idx]\n",
        "    if len(word_dict) == 0:\n",
        "        continue\n",
        "    sorted_words = sorted(word_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
        "    tokens_plot, values_plot = zip(*sorted_words)\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.barh(tokens_plot[::-1], [abs(v) for v in values_plot[::-1]],\n",
        "             color='skyblue' if class_idx==0 else 'salmon')\n",
        "    plt.title(f\"Integrated Gradients - Local - Classe {class_idx}\")\n",
        "    plt.xlabel(\"Attribution absolue\")\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Affichage console\n",
        "# -----------------------------\n",
        "for class_idx, word_dict in local_attr.items():\n",
        "    print(f\"\\nClasse {class_idx} - Top mots :\")\n",
        "    for word, val in sorted(word_dict.items(), key=lambda x: abs(x[1]), reverse=True):\n",
        "        print(f\"{word:15s} {abs(val):.4f}\")"
      ],
      "metadata": {
        "id": "G4UKRQ-zqPgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mise en oeuvre de Integrated gradients pour un jeu de donn√©es"
      ],
      "metadata": {
        "id": "W1arMkU2qZdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from captum.attr import IntegratedGradients\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Param√®tres\n",
        "# -----------------------------\n",
        "MODEL_PATH = \"/content/clinicalbert_diabetes\"\n",
        "DATA_PATH = \"/content/diabetes_clinical_notes.csv\"\n",
        "MAX_LENGTH = 256\n",
        "TOP_K = 30\n",
        "SAMPLE_SIZE = 10\n",
        "n_steps = 50\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Charger mod√®le et tokenizer\n",
        "# -----------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "ig = IntegratedGradients(lambda emb, mask: model(inputs_embeds=emb, attention_mask=mask).logits)\n",
        "embeddings = model.get_input_embeddings()\n",
        "\n",
        "# -----------------------------\n",
        "# Charger dataset\n",
        "# -----------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(subset=[\"TEXT\"])\n",
        "df[\"TEXT\"] = df[\"TEXT\"].astype(str)\n",
        "texts = df[\"TEXT\"].tolist()\n",
        "\n",
        "# √âchantillon repr√©sentatif\n",
        "sample_texts = random.sample(texts, min(SAMPLE_SIZE, len(texts)))\n",
        "\n",
        "# -----------------------------\n",
        "# Initialiser dictionnaires globaux\n",
        "# -----------------------------\n",
        "word_importance_class0 = defaultdict(float)\n",
        "word_importance_class1 = defaultdict(float)\n",
        "\n",
        "# -----------------------------\n",
        "# Boucle sur chaque texte\n",
        "# -----------------------------\n",
        "for text in sample_texts:\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\",\n",
        "        return_offsets_mapping=True\n",
        "    ).to(device)\n",
        "\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "    attention_mask = encoding[\"attention_mask\"]\n",
        "    offsets = encoding[\"offset_mapping\"][0].tolist()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    input_embeds = embeddings(input_ids)\n",
        "\n",
        "    for class_idx in range(model.config.num_labels):\n",
        "        attributions = ig.attribute(\n",
        "            inputs=input_embeds,\n",
        "            additional_forward_args=attention_mask,\n",
        "            target=class_idx,\n",
        "            n_steps=n_steps\n",
        "        ).squeeze(0)\n",
        "\n",
        "        token_attr = attributions.sum(dim=-1).detach().cpu().numpy()\n",
        "\n",
        "        # Agr√©gation par mot\n",
        "        word_dict = defaultdict(float)\n",
        "        current_word = \"\"\n",
        "        current_val = 0.0\n",
        "        previous_end = None\n",
        "\n",
        "        for token, (start, end), val in zip(tokens, offsets, token_attr):\n",
        "            if token in tokenizer.all_special_tokens or start == end:\n",
        "                continue\n",
        "            piece = text[start:end]\n",
        "            if previous_end is not None and start != previous_end:\n",
        "                word_clean = current_word.lower().strip()\n",
        "                if word_clean.isalpha() and word_clean not in stop_words and word_clean not in punctuation:\n",
        "                    word_dict[word_clean] += current_val\n",
        "                current_word = piece\n",
        "                current_val = val\n",
        "            else:\n",
        "                current_word += piece\n",
        "                current_val += val\n",
        "            previous_end = end\n",
        "\n",
        "        if current_word:\n",
        "            word_clean = current_word.lower().strip()\n",
        "            if word_clean.isalpha() and word_clean not in stop_words and word_clean not in punctuation:\n",
        "                word_dict[word_clean] += current_val\n",
        "\n",
        "        # Ajouter au dictionnaire global\n",
        "        if class_idx == 0:\n",
        "            for w, v in word_dict.items():\n",
        "                word_importance_class0[w] += abs(v)\n",
        "        else:\n",
        "            for w, v in word_dict.items():\n",
        "                word_importance_class1[w] += abs(v)\n",
        "\n",
        "# -----------------------------\n",
        "# Trier top mots\n",
        "# -----------------------------\n",
        "sorted_class0 = sorted(word_importance_class0.items(), key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "sorted_class1 = sorted(word_importance_class1.items(), key=lambda x: x[1], reverse=True)[:TOP_K]\n",
        "\n",
        "tokens0, values0 = zip(*sorted_class0)\n",
        "tokens1, values1 = zip(*sorted_class1)\n",
        "\n",
        "# -----------------------------\n",
        "# Visualisation\n",
        "# -----------------------------\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16,8))\n",
        "axes[0].barh(tokens0[::-1], values0[::-1], color='skyblue')\n",
        "axes[0].set_title(\"Top mots classe 0\")\n",
        "axes[0].set_xlabel(\"Importance globale (|IG|)\")\n",
        "axes[1].barh(tokens1[::-1], values1[::-1], color='salmon')\n",
        "axes[1].set_title(\"Top mots classe 1\")\n",
        "axes[1].set_xlabel(\"Importance globale (|IG|)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Tableau comparatif\n",
        "# -----------------------------\n",
        "df_compare = pd.DataFrame({\n",
        "    \"Classe 0 mots\": tokens0,\n",
        "    \"Classe 0 importance\": values0,\n",
        "    \"Classe 1 mots\": tokens1,\n",
        "    \"Classe 1 importance\": values1\n",
        "})\n",
        "print(df_compare)"
      ],
      "metadata": {
        "id": "CyinMwnbqYPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}